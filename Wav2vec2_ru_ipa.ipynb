{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wav2vec2 example\n",
        "Accentuation and transcription can be useful for acustic corpora analysis. This notebook contains an example of running the wav2vec2-lv-60-espeak-cv-ft model finetuned with [`RUSLAN`](https://ruslan-corpus.github.io/) and [`Common Voice`](https://commonvoice.mozilla.org/ru)\n"
      ],
      "metadata": {
        "id": "xEmXMe9SEjK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download model from Hugging Face\n",
        "!mkdir model\n",
        "!git clone https://huggingface.co/omogr/wav2vec2-lv-60-ru-ipa model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5ZEeq1LMH2E",
        "outputId": "ff112dc9-6292-4d44-b0c1-1227f760da7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'model'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 20 (delta 2), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (20/20), 304.43 KiB | 6.34 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import random\n",
        "\n",
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "from transformers import Wav2Vec2Processor\n",
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "MODEL_PATH = 'model'\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\n",
        "    \"model/vocab.json\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    word_delimiter_token=\"|\",\n",
        "    do_lower_case=False\n",
        ")\n",
        "\n",
        "# @title Load model and processor\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_PATH)\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "        MODEL_PATH,\n",
        "        attention_dropout=0.0,\n",
        "        hidden_dropout=0.0,\n",
        "        feat_proj_dropout=0.0,\n",
        "        mask_time_prob=0.0,\n",
        "        layerdrop=0.0,\n",
        "        gradient_checkpointing=True,\n",
        "        ctc_loss_reduction=\"mean\",\n",
        "        ctc_zero_infinity=True,\n",
        "        bos_token_id=processor.tokenizer.bos_token_id,\n",
        "        eos_token_id=processor.tokenizer.eos_token_id,\n",
        "        pad_token_id=processor.tokenizer.pad_token_id,\n",
        "        vocab_size=len(processor.tokenizer.get_vocab()),\n",
        "    )\n",
        "\n",
        "def process_wav_file(wav_file_path: str):\n",
        "    # read soundfiles\n",
        "    waveform, sample_rate = torchaudio.load(wav_file_path)\n",
        "\n",
        "    bundle_sample_rate = 16000\n",
        "    if sample_rate != bundle_sample_rate:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle_sample_rate)\n",
        "\n",
        "    # tokenize\n",
        "    input_values = processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "    # retrieve logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values.view(1, -1)).logits\n",
        "    # take argmax and decode\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    return processor.batch_decode(predicted_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1UCVSToMGVr",
        "outputId": "ba94419d-8401-4511-f33b-879f1d109777"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at model were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.masked_spec_embed']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_wav_files = [\n",
        "    'model/sample_wav_files/common_voice_ru_38488940.wav',\n",
        "    'model/sample_wav_files/common_voice_ru_38488941.wav',\n",
        "]\n",
        "\n",
        "# @title Transcribe wav files\n",
        "for wav_file_path in sample_wav_files:\n",
        "    print('File:', wav_file_path)\n",
        "    transcription = process_wav_file(wav_file_path)\n",
        "    print('Transcription:', transcription)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIFV210KM6tw",
        "outputId": "d8ab7223-8aa9-47d9-d17f-387eaa19880a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: model/sample_wav_files/common_voice_ru_38488940.wav\n",
            "Transcription: ['kak v tr`udnɨje tak i d`obrɨj vrʲɪmʲɪn`a n`aʂɨ məɫɐdʲ`ɵʂ `ɛtə ɡɫ`avnəjə bɐɡ`atstvə']\n",
            "File: model/sample_wav_files/common_voice_ru_38488941.wav\n",
            "Transcription: ['mɨ nɐdʲ`ejɪmsʲə ʂto fsʲe ɡəsʊd`arstvə pɐdʲː`erʐɨvəjɪt `ɛtət tʲekst pənʲɪm`ajɪt ʂto n`ɨnʲɪʂnʲɪjə bʲɪzʲdʲ`ejstvʲɪje nʲɪprʲɪ`jemlʲɪmə']\n"
          ]
        }
      ]
    }
  ]
}